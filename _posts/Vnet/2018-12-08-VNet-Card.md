---
layout: post
title:  "VNet-Card"
date:   2018-12-08 11:33:30 +0800
categories: [HW]
excerpt: VNet-Card.
tags:
  - Net
---

>**Architecture: ARM64 and X86(Ubuntu 14.04)**

>**Kernel: Linux 4.14**

**VNet-Card** 是没有网络物理层相连的两个设备之间，使用虚拟网卡进行网络数据交换
的项目。其目是用于没有直接网络物理设备相连的两个设备节点间，使用其他物理通道充
当网络物理层，最终达到两个设备节点之间可以进行网络数据交换。

根据 OSI 七层网络协议，网络协议在进行数据交换时，真实的数据需要在物理层进行数
据传输，然后传递给上层使用，如下图：

![OSI7](/assets/PDB/BiscuitOS/kernel/NET000100.png)

在 VNet-Card 需求中，由于没有直接的网络设备充当物理层，所有需要其他物理通路来
充当物理层。本教程中，将使用 PCIe/DMA 来充当物理层，其逻辑框架如下图：

![VNETCARD](/assets/PDB/BiscuitOS/kernel/NET000101.png)

正如上图所示，在 X86 主板上插入一块 FPGA 板子到 PCIe 卡槽上，因此 FPGA 与 X86 
之间以 PCIe 为物理通路。在 X86 和 FPGA 上分别创建两个 TUN 虚拟网卡。DMA 位于 
FPGA 上，用于内存与内存或设备之间进行数据的拷贝。两个 FIFO 队列，一个用于 
FPGA 读和 X86 写队列，另外一个用于 FPGA 写和 X86 读队列。通过网络工具，如 
ping, iperf 等可以通过虚拟网卡进行网络数据交换。网卡管理工具用于管理网卡数据的
发送和接受。以上为本例子中使用的硬件及软件工具，其工作流程如下：

1） 在 X86 和 FPGA 上分别创建 TUN 虚拟网卡，设置对应的 IP 和子网掩码。

2） FPGA 侧的网络工具将网络数据传输到 TUN 虚拟网卡，此时 FPGA 侧的网卡管理软
件将虚拟网卡传输下来的数据存储到共享内存的 FIFO 中，该 FIFO 用于 FPGA 写和 
X86 读。

3） X86 侧网卡管理软件得知 FIFO 有新数据之后，将新数据从 FIFO 中取出，然后通过 
DMA 和 PCIe 传输到 X86 侧的 TUN 虚拟网卡，TUN 虚拟网卡再将数据网上传给网络应用
工具和工具。

4） X86 侧网络应用程序将网络数据通过 TUN 虚拟网卡进行传输，X86 侧网卡管理程序
获得 TUN 传输下来的数据，然后将数据通过 CPIe 和 DMA 存储到 FPGA 共享内存的 
FIFO 中，该 FIFO 用于 FPGA 读和 X86 写。

5） FPGA 侧网卡管理程序得知 FIFO 有新数据之后，将新数据从 FIFO 中取出，然后通
过传递给 TUN 虚拟网卡。TUN 再将数据继续传递给上层的网络应用和工具。

# VNet-Card 实践

VNet-Card 相关的源码已经在 github 上开源，开发者可以根据本节教程进行编译使用。
版本在不断更新中，开发者可以跟进这篇文章进行优化和使用。

#### 源码

VNet-Card github 项目地址：

{% highlight ruby %}
https://github.com/WorkRoutine/VNet-Card
{% endhighlight %}

VNet-Card 项目源码获得和使用请按如下步骤：

{% highlight ruby %}
git clone https://github.com/WorkRoutine/VNet-Card.git
cd VNet-Card/
git reset --hard v1.0.0
{% endhighlight %}

文件目录如下：

![SRCTREE](/assets/PDB/BiscuitOS/kernel/NET000102.png)

> dma.c/dma.h: 用于 FPGA DMA/PCIe 操作
>
> fifo.c/fifo.c: 用于 FIFO 队列管理
>
> mem.c/mem.h: 用于共享内存管理
>
> tap.c/tap.h: 用于 TUN 虚拟网卡管理
>
> Vnet-card_FPGA.c: 用于 FPGA 虚拟网卡管理
>
> Vnet-card_X86.c: 用于 X86 虚拟网卡管理

#### 编译

由于虚拟网卡管理软件需要分别泡在 X86 和 ARM64，请参照如下步骤进行编译：

**X86 虚拟网卡**

X86 端编译之前，请先自行安装 GCC 工具链，具体编译过程请参考 Makefile。

{% highlight ruby %}
make
{% endhighlight %}

编译完之后，会在源码目录下生成一个名叫 VNet-Card_X86 的执行文件，开发者可以直
接运行或将其拷贝到系统工具库，如下步骤：

{% highlight ruby %}
sudo chmod 777 VNet-Card_X86
mv VNet-Card_X86 /usr/bin/
{% endhighlight %}

FPGA 虚拟网卡
FPGA 使用的是 ARM64， 开发者请准备好对应的交叉编译，具体可以参考 Makefile。

{% highlight ruby %}
export CC=CROSS_COMPILER
make
{% endhighlight %}

编译完之后，会在源码目录下生成一个名叫 VNet-Card_FGPA 的执行文件，开发者可以
直接运行或将其拷贝到系统工具库，如下步骤：

{% highlight ruby %}
sudo chmod 777 VNet-Card_FPGA
mv VNet-Card_FPGA /usr/bin/
{% endhighlight %}

#### 使用

X86 和 FPGA 上使用虚拟网卡都采用同样的步骤，基本流程可以参照标准网卡的使用方
法。具体步骤如下：

{% highlight ruby %}
./VNet-Card_X86 &
or
./VNet-Card_FPGA &
{% endhighlight %}

运行完上面的命令之后，开发者可以使用网络工具，例如 ifconfig，ip 等，这些工具可
以使用这个虚拟网卡。例如在 X86 上使用 ifconfig 工具查看虚拟网卡，如下：

![PING0](/assets/PDB/BiscuitOS/kernel/NET000103.png)

从上图可以看到，运行 VNet-Card_X86 程序之后，系统多了一个名叫 tun0 的网卡，默
认 ip 设置为 10.10.10.1，子网掩码默认设置为 255.255.255.0 。同理在 FPGA 上运行
VNet-Card_FPGA 程序之后，系统多了一个名叫 tun0 的网卡，默认 ip 设置为 
10.10.10.5，子网掩码默认设置为 255.255.255.0，如下图：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000104.png)

#### ping 测试

首先开发者可以使用 ping 命令进行基础测试，以此测试双方能否 ping 通，在 X86 上
使用如下命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10
{% endhighlight %}

![PING1](/assets/PDB/BiscuitOS/kernel/NET000105.png)

如上图测试结果，进行 10 次 ping 测试之后，丢包率为 0，说明传输 56 字节数据没有
问题。同理在 FPGA 也使用 ping 命令进行测试，如下：

{% highlight ruby %}
ping 10.10.10.1 -c 10
{% endhighlight %}

![PING1](/assets/PDB/BiscuitOS/kernel/NET000106.png)

通过上面测试，该方案证明可行，开发者可以参考这个方案进行自己的虚拟网卡开发。

---------------------------------------
# VNet-Card 技术原理分析

### TUN/TAP 虚拟网卡

{% highlight ruby %}
TUN/TAP 原理来源于网络，网址：https://segmentfault.com/a/1190000009249039
{% endhighlight %}

在 Linux 网络数据包的接收过程和数据包的发送过程这两篇文章中，介绍了数据包的收
发流程，知道了 Linux 内核中有一个网络设备管理层，处于网络设备驱动和协议栈之
间，负责衔接它们之间的数据交互。驱动不需要了解协议栈的细节，协议栈也不需要了解
设备驱动的细节。

对于一个网络设备来说，就像一个管道（pipe）一样，有两端，从其中任意一端收到的数
据将从另一端发送出去。比如一个物理网卡 eth0，它的两端分别是内核协议栈（通过内
核网络设备管理模块间接的通信）和外面的物理网络，从物理网络收到的数据，会转发给
内核协议栈，而应用程序从协议栈发过来的数据将会通过物理网络发送出去。

那么对于一个虚拟网络设备呢 ？首先它也归内核的网络设备管理子系统管理，对于 
Linux 内核网络设备管理模块来说，虚拟设备和物理设备没有区别，都是网络设备，都能
配置 IP，从网络设备来的数据，都会转发给协议栈，协议栈过来的数据，也会交由网络
设备发送出去，至于是怎么发送出去的，发到哪里去，那是设备驱动的事情，跟 Linux 
内核就没关系了，所以说虚拟网络设备的一端也是协议栈，而另一端是什么取决于虚拟网
络设备的驱动实现。

Tun/Tap

先看一下逻辑关系图

![PING1](/assets/PDB/BiscuitOS/kernel/NET000107.png)

上图中有两个应用程序 A 和 B，都在用户层，而其它的 socket、协议栈（Newwork 
Protocol Stack）和网络设备（eth0 和 tun0）部分都在内核层，其实 socket 是协议
栈的一部分，这里分开来的目的是为了看的更直观。

tun0 是一个 Tun/Tap 虚拟设备，从上图中可以看出它和物理设备eth0的差别，它们的一
端虽然都连着协议栈，但另一端不一样，eth0 的另一端是物理网络，这个物理网络可能
就是一个交换机，而 tun0 的另一端是一个用户层的程序，协议栈发给tun0的数据包能被
这个应用程序读取到，并且应用程序能直接向 tun0 写数据。这里假设 eth0 配置的IP是
10.32.0.11，而 tun0 配置的 IP 是 192.168.3.11.

这里列举的是一个典型的 tun/tap 设备的应用场景，发到 192.168.3.0/24 网络的数据
通过程序B这个隧道，利用 10.32.0.11 发到远端网络的 10.33.0.1，再由 10.33.0.1 转
发给相应的设备，从而实现 VPN。

下面来看看数据包的流程：

> 1. 应用程序A是一个普通的程序，通过 socket A 发送了一个数据包，假设这个数据包
>    的目的 IP 地址是 192.168.3.1
>
> 2. socket 将这个数据包丢给协议栈
>   
> 3. 协议栈根据数据包的目的IP地址，匹配本地路由规则，知道这个数据包应该由 
>    tun0 出去，于是将数据包交给 tun0
>
> 4. tun0 收到数据包之后，发现另一端被进程 B 打开了，于是将数据包丢给了进程 B
>   
> 5. 进程 B 收到数据包之后，做一些跟业务相关的处理，然后构造一个新的数据包，将原
>    来的数据包嵌入在新的数据包中，最后通过socket B 将数据包转发出去，这时候新数
>    据包的源地址变成了eth0 的地址，而目的IP地址变成了一个其它的地址，比如是 
>    10.33.0.1.
>   
> 6. socket B 将数据包丢给协议栈
>   
> 7. 协议栈根据本地路由，发现这个数据包应该要通过 eth0 发送出去，于是将数据包交
>    给 eth0
>   
> 8. eth0 通过物理网络将数据包发送出去
>   
> 9. 10.33.0.1 收到数据包之后，会打开数据包，读取里面的原始数据包，并转发给本
>    地的 192.168.3.1，然后等收到 192.168.3.1 的应答后，再构造新的应答包，并将
>    原始应答包封装在里面，再由原路径返回给应用程序 B，应用程序 B 取出里面的原
>    始应答包，最后返回给应用程序 A

这里不讨论 Tun/Tap 设备 tun0 是怎么和用户层的进程 B 进行通信的，对于 Linux 内
核来说，有很多种办法来让内核空间和用户空间的进程交换数据。从上面的流程中可以看
出，数据包选择走哪个网络设备完全由路由表控制，所以如果我们想让某些网络流量走应
用程序 B 的转发流程，就需要配置路由表让这部分数据走 tun0。

TUN/TAP 使用实例：

{% highlight ruby %}
https://github.com/BiscuitOS/HardStack/tree/master/bus/net/tun/app
{% endhighlight %}

### FIFO 管理队列

FIFO 用于管理网络数据的首发，采用两个标准的 FIFO 队列，第一个队列用于 X86 TUN 
网卡写数据，另一端用于 FPGA TUN 网卡读数据。第二个队列用于 FPGA TUN 网卡写数
据，另外一端用于 X86 TUN 网卡读数据。每个 FIFO 都采用读写锁进行数据同步。

FIFO 使用实例：

{% highlight ruby %}
https://github.com/BiscuitOS/HardStack/tree/master/bus/fifo/share_fifo
{% endhighlight %}

---------------------------------------------------
# VNet-Card 测试

本节用于描述 VNet-Card 的测试记录，每个测试都基于 git tag 的不同版本。

#### Ping 56 字节测试

VNet-Card 版本： V1.0.0

测试目的： 56 字节 ping 包测试，测试丢包率。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 54
{% endhighlight %}

X86 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000108.png)

FPGA 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000109.png)

总结： V1.0.0 版本上，ping 54 字节包丢包率为 0，符合预期

#### Ping 256 字节测试

VNet-Card 版本： V1.0.0

测试目的： 256 字节 ping 包测试，测试丢包率。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 256
{% endhighlight %}

X86 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000110.png)

FPGA 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000111.png)

总结： V1.0.0 版本上，ping 256 字节包丢包率为 0，符合预期

#### Ping 512 字节测试

VNet-Card 版本： V1.0.0

测试目的： 512 字节 ping 包测试，测试丢包率。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 512
{% endhighlight %}

X86 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000112.png)

FPGA 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000113.png)

总结： V1.0.0 版本上，ping 512 字节包丢包率为 0，符合预期

#### Ping 1024 字节测试

VNet-Card 版本： V1.0.0

测试目的： 1024 字节 ping 包测试，测试丢包率。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 1024
{% endhighlight %}

X86 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000114.png)

FPGA 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000115.png)

总结： V1.0.0 版本上，ping 1024 字节包丢包率为 0，符合预期

#### Ping 2048 字节测试

VNet-Card 版本： V1.0.0

测试目的： 2048 字节 ping 包测试，测试丢包率。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 2048 -t 20
{% endhighlight %}

X86 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000116.png)

FPGA 测试结果：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000117.png)

总结： V1.0.0 版本上，ping 2048 字节包丢包率为 100%. 存在 BUG， 需要修复。


----------------------------------------
# VNet-Card 优化记录

#### Ping 2048 字节，丢包率 100% 问题

VNet-Card 版本： V1.0.0

问题描述： 2048 字节 ping 包测试，丢包率 100%。

使用命令：

{% highlight ruby %}
ping 10.10.10.5 -c 10 -s 2048 -t 20
{% endhighlight %}

![PING1](/assets/PDB/BiscuitOS/kernel/NET000118.png)

面对这种情况，修改代码如下：

{% highlight ruby %}
diff --git a/Vnet-card_X86.c b/Vnet-card_X86.c
index f58f499..d3ef58f 100644
--- a/Vnet-card_X86.c
+++ b/Vnet-card_X86.c
@@ -168,7 +168,7 @@ void *send_procedure(void *arg)
         unsigned int eflags;
         /* Read from Tun/Tap */
-        count = read(tun_fd, WpBuf, 1200);
+        count = read(tun_fd, WpBuf, FIFO_BUFFER);
         eflags = wait_and_lock(X86_WR_FIFO_OFFSET);
         fifo_manage_get(Wbase, X86_WR_FIFO_OFFSET);



diff --git a/Vnet-card_FPGA.c b/Vnet-card_FPGA.c
index bf997a4..70c9459 100644
--- a/Vnet-card_FPGA.c
+++ b/Vnet-card_FPGA.c
@@ -170,7 +170,7 @@ void *send_procedure(void *arg)
         unsigned int eflags;
         /* Read from Tun/Tap */
-        count = read(tun_fd, WpBuf, 1200);
+        count = read(tun_fd, WpBuf, FIFO_BUFFER);
         /* Set Vnet-card Ready */
         eflags = wait_and_lock(ARM_WR_FIFO_OFFSET);
{% endhighlight %}

再进行测试， X86 测试结果如下：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000119.png)

FPGA 测试结果如下：

![PING1](/assets/PDB/BiscuitOS/kernel/NET000120.png)

测试结果显示丢包率 20%，该修改不能彻底解决问题，需要对架构进行优化。优化重点在
FIFO 队列的管理。

# 联系

如果发现有任何问题，请联系我

{% highlight ruby %}
buddy.zhang@aliyun.com
{% endhighlight %}
