---
layout: post
title:  "Open VnetCard 虚拟网卡"
date:   2019-06-25 05:30:30 +0800
categories: [HW]
excerpt: Vnet.
tags:
  - OpenProject
---

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/GIF000203.gif)

> [Github: Open VnetCard](https://github.com/WorkRoutine/VNet-Card)
>
> Email: BuddyZhang1 <buddy.zhang@aliyun.com>

# 目录

> - [VnetCard 原理](#A0)
>
> - [VnetCard 使用](#B0)
>
>   - [VnetCard 源码](#B00)
>
>   - [VnetCard 编译](#B01)
>
>   - [VnetCard 运行](#B02)
>
>   - [VnetCard 网关配置](#B03)
>
> - [VnetCard 实现](#C0)
>
>   - [Tun/Tap](#C00)
>
>   - [DMA](#C01)
>
>   - [Queue](#C02)
>
>   - [RingBuffer](#C03)
>
>   - [mmap](#C04)
>
>   - [Signal](#C05)
>
> - [VnetCard 测试](#D0)
>
>   - [ping](#D00)
>
>   - [ssh](#D01)
>
>   - [scp](#D02)
>
>   - [iperf TCP](#D03)
>
>   - [iperf UDP](#D04)
>
>   - [socket](#D05)
>
> - [VnetCard 优化](#E0)
>
> - [VnetCard 问题合集](#F0)
>
> - [附录](#附录)

-----------------------------------
<span id="A0"></span>

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/IND00000A.jpg)

## VnetCard 原理

Open VnetCard 虚拟网卡是一种在没有真实网卡存在的环境下，使用其他物理链路充当
网卡的数据链路，并通过 Linux 提供的 TUN/TAP 提供的接口实现的一种虚拟网卡。
Open VnetCard 项目中，虚拟网卡是将一块 FPGA/ARM 硬件平台通过 PCIe 插入到 X86
主机上，通过 PCIe 上的 DMA 和 queue 进行网络物理数据的交互，并使用 TUN/TAP 虚
拟网卡完成物理数据的网络协议解析，其实现框架如下图：

![](/assets/PDB/BiscuitOS/boot/BOOT000139.png)

VnetCard 虚拟网卡的实现原理并不复杂，X86 端的实现和 FPGA/ARM 端的实现逻辑基本
一直，但也存在细微差别，这里分别对两端的实现原理做介绍。

--------------------------------

### X86 原理

![](/assets/PDB/BiscuitOS/boot/BOOT000140.png)

###### VnetCard 发送数据

在 X86 端用户空间通过使用各种各样的网络工具，将数据通过 Socket 接口传递给
Linux 的网络协议层，网络协议层再将 socket 传递的数据进行封包，然后传递给
TUN/TAP 驱动。TUN/TAP 驱动在获得网络层传递下来的数据之后，将数据通过
字符设备，以字节流的方式传递给 TUN/TAP 在用户空间的接口，此时 VnetCard 使
用 VFS 为 TUN/TAP 提供的 open/read 接口从 TUN/TAP 中获得数据流，
获得数据流之后，将多个数据拼在一起放置在 RingBuf 中，当符合某个条件之后，
VnetCard 使用 DMA 将 RingBuffer 中准备好的数据经 PCIe 拷贝到 FPGA/ARM
物理内存的指定位置，在拷贝完毕后，VnetCard 通过 queue 告诉 FPGA/ARM 有
新的网络数据已经准备好。至此 X86 端 VnetCard 发送数据过程完毕。

###### VnetCard 接收数据

VnetCard 通过轮询的方式读取 queue 中的信息，以确定是否有新的数据到达。如果
queue 中有新数据的信息，VnetCard 就从 queue 中读取新数据在 FPGA/ARM 中的
信息，然后调用 DMA 从 FPGA/ARM 物理内存的指定位置上读取数据到 VnetCard
的 RingBuffer 中，接着 VnetCard 就将 RingBuffer 中的新数据进行拆包分离
成 TUN/TAP 识别的帧，然后将这些帧通过 VFS 提供的 open/write 接口，写入
到 TUN/TAP 在用户空间的接口，此时 TUN/TAP 驱动收到写入的数据，并将这些
数据传入到网络协议层，最后通过 Socket 传递给用户空间各种网络工具。至此
X86 端 VnetCard 接收数据过程完毕。

--------------------------------------

### FPGA/ARM 原理

![](/assets/PDB/BiscuitOS/boot/BOOT000141.png)

###### VnetCard 发送数据

在 FPGA/ARM 端用户空间通过使用各种各样的网络工具，将数据通过 Socket 接口传递给
Linux 的网络协议层，网络协议层再将 socket 传递的数据进行封包，然后传递给
TUN/TAP 驱动。TUN/TAP 驱动在获得网络层传递下来的数据之后，将数据通过
字符设备，以字节流的方式传递给 TUN/TAP 在用户空间的接口，此时 VnetCard 使
用 VFS 为 TUN/TAP 提供的 open/read 接口从 TUN/TAP 中获得数据流，
获得数据流之后，将多个数据拼在一起放置在 RingBuf 中，当符合某个条件之后，
VnetCard 通过 queue 告诉 X86 有新的网络数据已经准备好。至此 FPGA/ARM 端
VnetCard 发送数据过程完毕。

###### VnetCard 接收数据

VnetCard 通过轮询的方式读取 queue 中的信息，以确定是否有新的数据到达。如果
queue 中有新数据的信息，VnetCard 就从 queue 中读取新数据在 FPGA/ARM 中的
信息，然后 VnetCard 就从 RingBuffer 指定的位置，将新数据进行拆包分离
成 TUN/TAP 识别的帧，然后将这些帧通过 VFS 提供的 open/write 接口，写入
到 TUN/TAP 在用户空间的接口，此时 TUN/TAP 驱动收到写入的数据，并将这些
数据传入到网络协议层，最后通过 Socket 传递给用户空间各种网络工具。至此
FPGA/ARM 端 VnetCard 接收数据过程完毕。

VnetCard 安装完毕之后，可用使用 ifconfig 查看其网卡相关的信息，此时，
可以把 VnetCard 当做一个真实的网卡进行使用，如下图：

![](/assets/PDB/BiscuitOS/boot/BOOT000142.png)

-----------------------------------
<span id="B0"></span>

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/IND00000B.jpg)

## VnetCard 使用

Open VnetCard 虚拟网卡项目通过源码编译安装方式使用，开发者可按如下步骤进行
编译使用。

> - [VnetCard 源码](#B00)
>
> - [VnetCard 编译](#B01)
>
> - [VnetCard 运行](#B02)
>
> - [VnetCard 网关配置](#B03)

----------------------------------

#### <span id="B00">VnetCard 源码</span>

> [实践源码 VnetCard GitHub](https://github.com/WorkRoutine/VNet-Card)

开发者也可以使用如下命令获得：

{% highlight bash %}
git clone https://github.com/WorkRoutine/VNet-Card.git
{% endhighlight %}

--------------------------------------

#### <span id="B01">VnetCard 编译</span>

由于 VnetCard FGPA/ARM 端程序是要运行在 ARM 上的，开发者应该准备好
对应的交叉编译工具，可以参考如下 Makefile 节本进行适配：

{% highlight bash %}
HOST_CC=gcc
CROSS_CC=BiscuitOS/output/linux-newest-arm64/aarch64-linux-gnu/aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc

## CFLAGS
CFLAGS += -I./ -lpthread

#CONFIG_ALL := -DCONFIG_SOCKET_DEBUG
SRC := main.c base.c signal.c

# CONFIG_DMA_QUEUE
#
SRC        += queue.c tap_tun.c dma.c

# CONFIG_

all: Vnet_Host Vnet_FPGA

Vnet_Host: $(SRC)
	@$(HOST_CC) $(SRC) $(CFLAGS) -DCONFIG_HOST $(CONFIG_ALL) -o $@

Vnet_FPGA: $(SRC)
	@$(CROSS_CC) $(SRC) $(CFLAGS) -DCONFIG_FPGA $(CONFIG_ALL) -o $@

clean:
	@rm -rf Vnet_*
{% endhighlight %}

在上面的 Makefile 中，需要使用 CROSS_CC 指定交叉编译工具的位置。交叉
编译工具设置完毕之后，使用如下命令进行编译：

{% highlight bash %}
make
{% endhighlight %}

编译通过之后，源码会生成两个执行文件 "Vnet_Host" 与 "Vnet_FPGA"，其中
Vnet_Host 是运行在 X86 端的执行文件，Vnet_FPGA 是运行在 FPGA/ARM 端的，
开发者需要将 Vnet_FPGA 拷贝到 FPGA/ARM 上。

--------------------------------------

#### <span id="B02">VnetCard 运行</span>

在 X86 和 FPGA/ARM 上准备好可执行文件之后，在后台运行可执行文件，如下：

{% highlight bash %}
X86:
$> ./Vnet_Host &

FPGA/ARM:
$> ./Vnet_FPGA &
{% endhighlight %}

运行完毕之后，使用 ifconfig 查看网卡的信息，此时网卡的名字为 tun0。如下图：

![](/assets/PDB/BiscuitOS/boot/BOOT000142.png)

--------------------------------------

#### <span id="B03">VnetCard 网关配置</span>

VnetCard 搭建好两端的网络通信之后，需要让两端都能访问外部网络，
此时，一端存在与外界的连接的真实物理网卡，因此可以在该段搭建一个
网桥或 NAT，以此将另外一端的网络数据包进行转发，这样两端的虚拟网卡
就能与外部网络通信。具体步骤参考如下：

##### X86

X86 端存在一个真实的物理网卡连接外部网络，因此在 X86 端搭建一个网桥
或者 NAT，专门用于转发 FPGA/ARM 的网络数据包。使用如下配置：

{% highlight bash %}
#!/bin/sh

## X86 NAT Configuration

# VnetCard name
landev=tun0
# Hardware Network Card Name
wandev=eno1
# Netmask for VnetCard
subnet=10.10.10.0/24
# Hardware Network Card IP
wanip=172.16.28.90

# NAT Configure
echo 1 > /proc/sys/net/ipv4/ip_forward
iptables -F
iptables -X
iptables -Z
iptables -t nat -A POSTROUTING -o $wandev -j MASQUERADE
{% endhighlight %}

###### FPGA/ARM

FPGA/ARM 端只存在虚拟网卡，此时 FPGA/ARM 发送给外部的网络数据包，
先经过 VnetCard 发送到 X86 端，然后通过 X86 端的 NAT 进行转发。
FPGA/ARM 端使用的配置命令如下：

{% highlight bash %}
route add default gw 10.10.10.1
{% endhighlight %}

通过上面的命令将 FPGA/ARM 的网关设置为 X86 端 VnetCard 的 IP 地址。
经过上面两端的配置，现在在 FPGA/ARM 端使用 ping 命令访问外网，如下图：

![](/assets/PDB/BiscuitOS/boot/BOOT000191.png)

-----------------------------------
<span id="C0"></span>

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/IND00000Q.jpg)

## VnetCard 实现

了解 VnetCard 的原理之后，本节重点介绍技术实现的细节。通过模块化的编程思想，
VnetCard 虚拟网卡由："Tun/Tap"、"DMA"、"Queue"、"RingBuffer"、"mmap"、"Signal"
几个部分构成，每个独立模块提供了可靠的功能，让 VnetCard 项目更加稳健和安全。
一下是每个模块实现的具体细节：

> - [Tun/Tap](#C00)
>
> - [DMA](#C01)
>
> - [Queue](#C02)
>
> - [RingBuffer](#C03)
>
> - [mmap](#C04)
>
> - [Signal](#C05)

-------------------------------------

#### <span id="C00">Tun/Tap</span>

在 Linux 网络数据包的接收过程和数据包的发送过程中，知道了 Linux 内核中有一个
网络设备管理层，处于网络设备驱动和协议栈之间，负责衔接它们之间的数据交互。驱动
不需要了解协议栈的细节，协议栈也不需要了解设备驱动的细节。

对于一个网络设备来说，就像一个管道 (pipe) 一样，有两端，从其中任意一端收到的数
据将从另一端发送出去。比如一个物理网卡 eth0，它的两端分别是内核协议栈（通过内
核网络设备管理模块间接的通信）和外面的物理网络，从物理网络收到的数据，会转发给
内核协议栈，而应用程序从协议栈发过来的数据将会通过物理网络发送出去。

那么对于一个虚拟网络设备呢? 首先它也归内核的网络设备管理子系统管理，对于
Linux 内核网络设备管理模块来说，虚拟设备和物理设备没有区别，都是网络设备，都能
配置 IP，从网络设备来的数据，都会转发给协议栈，协议栈过来的数据，也会交由网络
设备发送出去，至于是怎么发送出去的，发到哪里去，那是设备驱动的事情，跟 Linux
内核就没关系了，所以说虚拟网络设备的一端也是协议栈，而另一端是什么取决于虚拟网
络设备的驱动实现。

对于 TUN/TAP 先看一下逻辑关系图

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/NET000107.png)

上图中有两个应用程序 A 和 B，都在用户层，而其它的 socket、协议栈（Newwork
Protocol Stack）和网络设备（eth0 和 tun0）部分都在内核层，其实 socket 是协议
栈的一部分，这里分开来的目的是为了看的更直观。

tun0 是一个 Tun/Tap 虚拟设备，从上图中可以看出它和物理设备eth0的差别，它们的一
端虽然都连着协议栈，但另一端不一样，eth0 的另一端是物理网络，这个物理网络可能
就是一个交换机，而 tun0 的另一端是一个用户层的程序，协议栈发给tun0的数据包能被
这个应用程序读取到，并且应用程序能直接向 tun0 写数据。这里假设 eth0 配置的IP是
10.32.0.11，而 tun0 配置的 IP 是 192.168.3.11.

这里列举的是一个典型的 tun/tap 设备的应用场景，发到 192.168.3.0/24 网络的数据
通过程序B这个隧道，利用 10.32.0.11 发到远端网络的 10.33.0.1，再由 10.33.0.1 转
发给相应的设备，从而实现 VPN。

下面来看看数据包的流程：

> 1. 应用程序A是一个普通的程序，通过 socket A 发送了一个数据包，假设这个数据包
>    的目的 IP 地址是 192.168.3.1
>
> 2. socket 将这个数据包丢给协议栈
>
> 3. 协议栈根据数据包的目的IP地址，匹配本地路由规则，知道这个数据包应该由
>    tun0 出去，于是将数据包交给 tun0
>
> 4. tun0 收到数据包之后，发现另一端被进程 B 打开了，于是将数据包丢给了进程 B
>
> 5. 进程 B 收到数据包之后，做一些跟业务相关的处理，然后构造一个新的数据包，将原
>    来的数据包嵌入在新的数据包中，最后通过socket B 将数据包转发出去，这时候新数
>    据包的源地址变成了eth0 的地址，而目的IP地址变成了一个其它的地址，比如是
>    10.33.0.1.
>
> 6. socket B 将数据包丢给协议栈
>
> 7. 协议栈根据本地路由，发现这个数据包应该要通过 eth0 发送出去，于是将数据包交
>    给 eth0
>
> 8. eth0 通过物理网络将数据包发送出去
>
> 9. 10.33.0.1 收到数据包之后，会打开数据包，读取里面的原始数据包，并转发给本
>    地的 192.168.3.1，然后等收到 192.168.3.1 的应答后，再构造新的应答包，并将
>    原始应答包封装在里面，再由原路径返回给应用程序 B，应用程序 B 取出里面的原
>    始应答包，最后返回给应用程序 A

这里不讨论 Tun/Tap 设备 tun0 是怎么和用户层的进程 B 进行通信的，对于 Linux 内
核来说，有很多种办法来让内核空间和用户空间的进程交换数据。从上面的流程中可以看
出，数据包选择走哪个网络设备完全由路由表控制，所以如果我们想让某些网络流量走应
用程序 B 的转发流程，就需要配置路由表让这部分数据走 tun0。

> - [Tun/Tap 使用实例](https://github.com/BiscuitOS/HardStack/tree/master/bus/net/tun/app)
>
> - [Tun/Tap on VnetCard](https://github.com/WorkRoutine/VNet-Card/blob/master/tap_tun.c)


-------------------------------------

#### <span id="C01">DMA</span>

DMA 是一种不经过 CPU 而直接从内存存取数据的数据交换模式。在 DMA 模式下，CPU
只须向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信
息反馈给 CPU，这样就很大程度上减轻了 CPU 资源占有率，可以大大节省系统资源。
DMA 模式又可以分为 Single-Word DMA（单字节DMA）和 Multi-Word DMA（多字节
DMA）两种，其中所能达到的最大传输速率也只有16.6MB/s。

在 VnetCard 虚拟网卡项目中，DMA 相关的操作位于 dma.c 中，其工作在 X86 端，
赋值 X86 端物理内存数据和 FPGA/ARM 物理内存数据的交换。dma.c 提供了简便的
接口，用于 DMA 的 read/write 操作。其使用方法可以参考下面文件：

> [DMA userspace API](https://github.com/WorkRoutine/VNet-Card/blob/master/dma.c)

-------------------------------------

#### <span id="C02">Queue</span>

Quene 是一个基于 PCIe 的 32bit 硬件队列，VnetCard 虚拟网卡使用 Queue 队列
的性质，用于传递数据块基本信息。虚拟网卡在使用 DMA 传递大块数据之后，都会使用
queue 通知对方；虚拟网卡也通过查询 queue 里面的消息，以此确定是否有新的数据
到达，这样就实现了多 CPU 之间的异步通知机制。

VnetCard 也对 queue 的信息帧实行的编码，编码方案有两种，分贝如下：

![](/assets/PDB/BiscuitOS/boot/BOOT000143.png)

两种方案都可行，具体使用哪种依实践情况而定，queue 的基本思路都是从
queue 队列中读取 32bit，然后判断这个 32bit 的 Magic 是否符合要求，
如果不符合要求，那么丢弃该帧，继续读取下一帧；如果 Magic 符合要求，
那么继续从 queue 中读取 Ringbuffer index，即新数据位于 Ringbuffer
的位置，接着也从 queue 中获得该 buffer 中一共含有多少个网络帧。VnetCard
从 queue 里面获得这些数据之后，通知对方 CPU，以此实现多 CPU 异步通知
机制。VnetCard 虚拟网卡中，queue 的实现过程请看下面链接：

> [Queue exchange frame message](https://github.com/WorkRoutine/VNet-Card/blob/master/queue.c)

-------------------------------------

#### <span id="C03">RingBuffer</span>

VnetCard 提供了一个 Ringbuffer 用于存储网络数据，Ringbuffer 被分成两部分，
第一部分用于存储发送的网络数据，第二部分用于存储接收的网络数据。每部分又被拆分
成更小的数据块，用于 DMA 的直接传送。如下图：

![](/assets/PDB/BiscuitOS/boot/BOOT000144.png)

###### Send Frame Buffer

Send Frame Buffer 用于存储从 Tun/Tap 虚拟网卡获得的网络帧，VnetCard 将
这些网络帧进行封包，封包之后再将包拼到 RingBuffer 的一块内存里。每当符合
特定条件时，VnetCard 虚拟网卡就将拼好的内存通过 DMA 发送到对端的内存上。

###### Receive Frame Buffer

Receive Frame Buffer 用于从对端内存中，通过 DMA 拷贝过来的数据块，数据
块里面都是拼号的帧。

RingBuffer 通过连续不断的将数据帧拼装到 Buffer 里面，然后等待时机成熟
进行发送，然后又循环到下一块 Buffer 继续拼数据包，当循环到最后一块数据
块之后，RingBuffer 有跳转到第一块继续循环。

###### 封包

VnetCard 虚拟网卡通过特定的规则将数据帧封包拼装到 Buffer 里面，封包的
方案有很多种，目前使用的是下图的方案：

![](/assets/PDB/BiscuitOS/boot/BOOT000145.png)

封包的过程很简约，首先在包头写入 Frame Magic 信息，然后写入数据包的长度，
后面紧跟着就是数据，最后每个包都是按 4 字节对齐，以提高效率。VnetCard
虚拟网卡中，封包相关的函数位于 dma.c 中，其使用 `dma_buffer_fill()` 函数。

###### 拆包

VnetCard 虚拟网卡从对端接收数据块之后，需要将数据块拆成 TUN/TAP 所
识别的帧，拆包的逻辑和封包相反，首先 VnetCard 从 Buffer 开始查找第一个
符合 Frame Magic 的位置，然后从中读到帧长度，并从中差分处数据帧到指定
位置，然后又查找下一个符合 Frame Magic 的位置，依次将数据帧解析完。
拆包的函数位于 dma.c 中，其使用 `dma_buffer_split()` 函数。

-------------------------------------

#### <span id="C04">mmap</span>

mmap 主要用于将物理地址映射到虚拟地址，在 VnetCard 中为了加速对物理地址的
范围，将 FPGA/ARM 的共享内存映射到 VnetCard 的虚拟空间，这大大方面的数据
的读取和交互。

-------------------------------------

#### <span id="C05">Signal</span>

VnetCard 虚拟网卡增加了对信号的处理，以防止在特殊情况下虚拟网卡部分功能出错
的情况下，其他功能能够安全的退出。相关的代码请看下面：

> - [VnetCard 虚拟网卡信号处理](https://github.com/WorkRoutine/VNet-Card/blob/master/signal.c)

-----------------------------------
<span id="D0"></span>

![](https://gitee.com/BiscuitOS/GIFBaseX/raw/master/RPI/IND00000E.jpg)

## VnetCard 测试

VnetCard 虚拟网卡可以使用通用的网络工具进行测试，下面就使用通过
的工具进行测试：

> - [ping](#D00)
>
> - [ssh](#D01)
>
> - [scp](#D02)
>
> - [iperf TCP](#D03)
>
> - [iperf UDP](#D04)
>
> - [socket](#D05)

-------------------------------------

#### <span id="D00">ping</span>

ping 作为最基础的测试，用于测试网络的连通性，丢包率测试，下面将
对 VnetCard 虚拟网卡使用不同的测试用例。X64 端 IP: 10.10.10.1。
FGPA/ARM 端 IP: 10.10.10.5。

> - [基础 ping 测试](#D000)
>
> - [1024 Byte](#D001)
>
> - [8192 Byte](#D002)
>
> - [10240 Byte](#D003)
>
> - [20480 Byte](#D004)
>
> - [超级大包长时间 ping 测试](#D005)
>
> - [超级小包长时间 ping 测试](#D006)

----------------------------

###### <span id="D000">基础 ping 测试</span>

测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 11
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000146.png)

X86 端使用 ping 命令，丢包率为 0%，平均每次 ping 耗时 2.004 毫秒。

![](/assets/PDB/BiscuitOS/boot/BOOT000147.png)

FPGA/ARM 段使用 ping 命令，丢包率 0%，平均每次 ping 耗时 2.663 毫秒。

-----------------------------------

###### <span id="D001">1024 Byte</span>

每个 ping 包大小 1024 个字节，测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 11 -s 1024
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000148.png)

X86 端使用 ping 命令，每次 1024 字节数据，丢包率为 0%，平均每次 ping
耗时 2.164 毫秒。

![](/assets/PDB/BiscuitOS/boot/BOOT000149.png)

FPGA/ARM 段使用 ping 命令，每次 1024 字节数据，丢包率 0%，平均每次
ping 耗时 1.892 毫秒。

-----------------------------------

###### <span id="D002">8192 Byte</span>

每个 ping 包大小 8192 个字节，测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 11 -s 8192
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000150.png)

X86 端使用 ping 命令，每次 8192 字节数据，丢包率为 0%，平均每次 ping
耗时 2.059 毫秒。

![](/assets/PDB/BiscuitOS/boot/BOOT000151.png)

FPGA/ARM 段使用 ping 命令，每次 8192 字节数据，丢包率 0%，平均每次
ping 耗时 2.281 毫秒。

------------------------------

###### <span id="D003">10240 Byte</span>

每个 ping 包大小 10240 个字节，测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 11 -s 10240
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000152.png)

X86 端使用 ping 命令，每次 10240 字节数据，丢包率为 0%，平均每次 ping
耗时 2.156 毫秒。

![](/assets/PDB/BiscuitOS/boot/BOOT000153.png)

FPGA/ARM 段使用 ping 命令，每次 10240 字节数据，丢包率 0%，平均每次
ping 耗时 2.215 毫秒。

----------------------------------

###### <span id="D004">20480 Byte</span>

每个 ping 包大小 20480 个字节，测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 11 -s 20480
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000154.png)

X86 端使用 ping 命令，每次 20480 字节数据，丢包率为 0%，平均每次 ping
耗时 2.767 毫秒。

![](/assets/PDB/BiscuitOS/boot/BOOT000155.png)

FPGA/ARM 段使用 ping 命令，每次 20480 字节数据，丢包率 0%，平均每次
ping 耗时 2.902 毫秒。

--------------------------------------

###### <span id="D005">超级大包长时间 ping 测试</span>

每个 ping 包大小 20480 个字节，测试 10 min 测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 600 -s 20480 -i 1
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000156.png)

X86 端使用 ping 命令，每次 20480 字节数据，丢包率为 0%，平均每次 ping
耗时 2.488 毫秒。600 次波形图如下：

![](/assets/PDB/BiscuitOS/boot/BOOT000157.png)

###### 超级小包长时间 ping 测试

每个 ping 包大小 20 个字节，测试 10 min 测试命令：

{% highlight bash %}
ping 10.10.10.5 -c 600 -s 20 -i 1
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000158.png)

X86 端使用 ping 命令，每次 20480 字节数据，丢包率为 0%，平均每次 ping
耗时 2.062 毫秒。600 次波形图如下：

![](/assets/PDB/BiscuitOS/boot/BOOT000159.png)

-------------------------------------

#### <span id="D01">ssh</span>

SSH 为 Secure Shell 的缩写，由 IETF 的网络小组（Network Working Group）所
制定；SSH 为建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和
其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄
露问题。SSH 最初是 UNIX 系统上的一个程序，后来又迅速扩展到其他操作平台。SSH 在
正确使用时可弥补网络中的漏洞。SSH 客户端适用于多种平台。几乎所有UNIX平台—包括
HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他平台，都可运行 SSH。

X86 端使用 SSH 登录对端，使用命令：

{% highlight bash %}
ssh root@10.10.10.10.5
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000160.png)

FPGA/ARM 端使用 SSH 登录对端，使用命令：

{% highlight bash %}
ssh test@10.10.10.10.1
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000161.png)

-------------------------------------

#### <span id="D02">scp</span>

scp 是 secure copy 的简写，用于在 Linux 下进行远程拷贝文件的命令，和它类似
的命令有 cp，不过 cp 只是在本机进行拷贝不能跨服务器，而且 scp 传输是加密的。
可能会稍微影响一下速度。当你服务器硬盘变为只读 read only system 时，用 scp
可以帮你把文件移出来。另外，scp 还非常不占资源，不会提高多少系统负荷，在这一
点上，rsync 就远远不及它了。虽然 rsync 比 scp 会快一点，但当小文件众多的情
况下，rsync 会导致硬盘I/O非常高，而 scp 基本不影响系统正常使用。

本机使用 scp 拷贝文件进行测试，测试如下：X86 端的 IP：10.10.10.1，
FPGA/ARM 端的 IP：10.10.10.5

> - [1M](#D020)
>
> - [10M](#D021)
>
> - [50M](#D022)
>
> - [100M](#D023)
>
> - [200M](#D024)
>
> - [20 次 200M](#D025)

-------------------------------

### <span id="D020">1M</span>

使用 scp 拷贝 1M 的文件，使用命令：

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000162.png)

X86 端拷贝速度为：1MB/s

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000163.png)

FPGA/ARM 端拷贝速度为：1MB/s

-------------------------------

### <span id="D021">10M</span>

使用 scp 拷贝 10M 的文件，使用命令：

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000164.png)

X86 端拷贝速度为：10MB/s

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000165.png)

FPGA/ARM 端拷贝速度为：5MB/s

-------------------------------

### <span id="D022">50M</span>

使用 scp 拷贝 50M 的文件，使用命令：

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000166.png)

X86 端拷贝速度为：8.3MB/s

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000167.png)

FPGA/ARM 端拷贝速度为：7.1MB/s

-------------------------------

### <span id="D023">100M</span>

使用 scp 拷贝 100M 的文件，使用命令：

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000168.png)

X86 端拷贝速度为：7.7MB/s

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000169.png)

FPGA/ARM 端拷贝速度为：5.6MB/s

-------------------------------

### <span id="D024">200M</span>

使用 scp 拷贝 200M 的文件，使用命令：

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000170.png)

X86 端拷贝速度为：8.1MB/s

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000171.png)

FPGA/ARM 端拷贝速度为：8.1MB/s

-------------------------------

### <span id="D025">20 次 200M</span>

使用 scp 拷贝 200M 的文件 20 次，使用命令：

###### X86 端

{% highlight bash %}
scp M.img root@10.10.10.5:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000172.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000173.png)

###### FPGA/ARM 端

{% highlight bash %}
scp M.img test@10.10.10.1:/tmp
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000174.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000175.png)

-------------------------------------

#### <span id="D03">iperf TCP</span>

iperf 是一个网络性能测试工具。iperf可以测试最大 TCP 和 UDP 带宽性能。iperf
具有多种参数和UDP特性，可以根据需要调整。iperf 可以报告带宽，延迟抖动和数据
包丢失。

iperf TCP 可用于测试：1）测量网络带宽。2）报告 MSS/MTU 值的大小和观测值。
3）支持 TCP 窗口值通过套接字缓冲。 4）当 P 线程或 Win3 2线程可用时，
支持多线程。客户端与服务端支持同时多重连接。本节用于测试 TCP 相关性能。
X64 端 IP: 10.10.10.1。FGPA/ARM 端 IP: 10.10.10.5。测试项目如下：

> - [TCP 性能测试](#D030)
>
> - [TCP 800M 带宽 5 小时测试](#D031)
>
> - [TCP 双向测试](#D032)

-------------------------------

#### <span id="D030">TCP 性能测试</span>

iperf 测试，X86 端作为 server，FPGA/ARM 端作为 client，使用的测试
命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.1 -t 5
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000176.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000177.png)

iperf 测试，FPGA/ARM 端作为 server，X86 端作为 client, 使用的测试命令
如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.5 -t 5
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000178.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000179.png)

-------------------------------

# <span id="D031">TCP 800M 带宽 5 小时测试</span>

iperf 测试，X86 端作为 server，FPGA/ARM 端作为 client，使用的测试
命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.1 -t 18000 -b 800M
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000180.png)

x86

![](/assets/PDB/BiscuitOS/boot/BOOT000181.png)

FPGA/ARM

-------------------------------

# <span id="D032">TCP 双向测试</span>

iperf 测试，X86 端作为 server，FPGA/ARM 端作为 client，使用的测试
命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.1 -t 20 -b 400M -d
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000182.png)

x86

![](/assets/PDB/BiscuitOS/boot/BOOT000183.png)

FPGA/ARM

-------------------------------------

#### <span id="D04">iperf UDP</span>

iperf 是一个网络性能测试工具。iperf可以测试最大 TCP 和 UDP 带宽性能。iperf
具有多种参数和UDP特性，可以根据需要调整。iperf 可以报告带宽，延迟抖动和数据
包丢失。

iperf TCP 可用于测试：1）客户端可以创建指定带宽的 UDP 流。2）测量丢包。
3）测量延迟。4）支持多播。5）当P线程可用时，支持多线程。
X64 端 IP: 10.10.10.1。FGPA/ARM 端 IP: 10.10.10.5。测试项目如下：

> - [UDP 性能测试](#D040)
>
> - [UDP 带宽测试](#D041)
>
> - [UDP 双向测试](#D042)
>
> - [UDP 丢包测试](#D042)

-------------------------------

# <span id="D040"></span>

iperf 测试，X86 端作为 server，FPGA/ARM 端作为 client，使用的测试
命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.1 -t 5 -u -b 400M
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000184.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000185.png)

iperf 测试，FPGA/ARM 端作为 server，X86 端作为 client, 使用的测试命令
如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.5 -t 5 -u -b 400M
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000186.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000187.png)

-------------------------------

# <span id="D041">UDP 带宽测试</span>

iperf 测试，X86 端作为 server，FPGA/ARM 端作为 client，最大带宽定义
为开始丢包的时候，使用的测试命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.1 -t 5 -u -b MaxM
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000188.png)

![](/assets/PDB/BiscuitOS/boot/BOOT000189.png)

从上面的测试可以看出，带宽可满足 1500M。

iperf 测试，FPGA/ARM 端作为 server，X86 端作为 client，最大带宽定义
为开始丢包的时候，使用的测试命令如下：

{% highlight bash %}
Server:
  iperf3 -s

Client:
  iperf3 -c 10.10.10.5 -t 5 -u -b MaxM
{% endhighlight %}

![](/assets/PDB/BiscuitOS/boot/BOOT000190.png)

-------------------------------------

#### <span id="D05">Socket</span>

Socket 的英文原义是 “孔” 或 “插座”。作为 BSD UNIX 的进程通信机制，取后一种
意思。通常也称作"套接字"，用于描述IP地址和端口，是一个通信链的句柄，可以用来
实现不同虚拟机或不同计算机之间的通信。在 Internet 上的主机一般运行了多个服务
软件，同时提供几种服务。每种服务都打开一个 Socket，并绑定到一个端口上，不同的
端口对应于不同的服务。Socket 正如其英文原义那样，像一个多孔插座。一台主机犹如
布满各种插座的房间，每个插座有一个编号，有的插座提供 220 伏交流电， 有的提供
110 伏交流电，有的则提供有线电视节目。 客户软件将插头插到不同编号的插座，就可
以得到不同的服务。X64 端 IP: 10.10.10.1。FGPA/ARM 端 IP: 10.10.10.5。

> - [Socket TCP 不定长测试](#D050)

-------------------------------

# <span id="D050">Socket TCP 不定长测试</span>

Socket 不定长测试是用于测试长度随机的网络数据包传输。测试方法是在一端开启
服务器端 socket 程序，另外一端开启客户端 socket 程序。测试程序如下：

> [TCP server 端测试程序](https://github.com/BiscuitOS/HardStack/blob/master/CodeSegment/socket/TCP/tcp_server.c)
>
> [TCP client 端测试程序](https://github.com/BiscuitOS/HardStack/blob/master/CodeSegment/socket/TCP/tcp_client.c)

-----------------------------------------------

# <span id="附录">附录</span>

> [BiscuitOS Home](https://biscuitos.github.io/)
>
> [BiscuitOS Driver](https://biscuitos.github.io/blog/BiscuitOS_Catalogue/)
>
> [BiscuitOS Kernel Build](https://biscuitos.github.io/blog/Kernel_Build/)
>
> [Linux Kernel](https://www.kernel.org/)
>
> [Bootlin: Elixir Cross Referencer](https://elixir.bootlin.com/linux/latest/source)
>
> [搭建高效的 Linux 开发环境](https://biscuitos.github.io/blog/Linux-debug-tools/)

## 赞赏一下吧 🙂

![MMU](/assets/PDB/BiscuitOS/kernel/HAB000036.jpg)
