---
layout: post
title:  "spinlock"
date:   2019-05-08 09:02:30 +0800
categories: [HW]
excerpt: spinlock().
tags:
  - SPINLOCK
---

![DTS](/assets/PDB/BiscuitOS/kernel/IND00000S.jpg)

> [Github: spinlock](https://github.com/BiscuitOS/HardStack/tree/master/Algorithem/spinlock/)
>
> Email: BuddyZhang1 <buddy.zhang@aliyun.com>
>
> Architecture: ARMv7 Cortex A9-MP

# 目录

> - [spinlock 锁原理](#spinlock 锁原理)
>
> - [spinlock 最小实践](#spinlock 最小实践)
>
> - [实践](#实践)
>
> - [附录](#附录)

-----------------------------------
<span id="spinlock 锁原理"></span>

![DTS](/assets/PDB/BiscuitOS/kernel/IND00000H.jpg)

# spinlock 锁原理

> - [spinlock 锁](#spinlock 锁)
>
> - [ARMv7 中的 spinlock 锁](#ARMv7 中的 spinlock 锁)
>
> - [WFI/WFE/SEV 指令](#WFI/WFE/SEV 指令)
>
> - [STREX/LDREX 指令](#STREX/LDREX 指令)

### <span id="spinlock 锁">spinlock 锁</span>

> [本节原理出自蜗窝科技，原文请戳](http://www.cnblogs.com/tureno/articles/6067441.html)

在 linux kernel 的实现中，经常会遇到这样的场景：共享数据被中断上下文和进程上下文访问，
该如何保护呢？如果只有进程上下文的访问，那么可以考虑使用 semaphore 或者 mutex 的锁机
制，但是现在中断上下文也参和进来，那些可以导致睡眠的 lock 就不能使用了，这时候，可以
考虑使用 spin lock。由于 spin lock 是 Architecture dependent 代码，因此本文介绍
ARMv7 上的实现。

##### spin lock 的特点

总结 spin lock 的特点如下：

{% highlight ruby %}
1) spin lock 是一种死等的锁机制。当发生访问资源冲突的时候，可以有两个选择：一个是
   死等，一个是挂起当前进程，调度其他进程执行。spin lock 是一种死等的机制，当前的
   执行 thread 会不断的重新尝试直到获取锁进入临界区。

2) 只允许一个 thread 进入。semaphore 可以允许多个 thread 进入，spin lock 不行,
   一次只能有一个 thread 获取锁并进入临界区，其他的 thread 都是在门口不断的尝试。
   在 ARMv7 上所谓死等就是让处理器进入 low-power state。

3) 执行时间短。由于 spin lock 死等这种特性，因此它使用在那些代码不是非常复杂的临
   界区（当然也不能太简单，否则使用原子操作或者其他适用简单场景的同步机制就 OK 了),
   如果临界区执行时间太长，那么不断在临界区门口 “死等” 的那些 thread 是多么的浪费
   CPU (当然，现代 CPU 的设计都会考虑同步原语的实现，例如 ARM 提供了 WFE 和 SEV
   这样的类似指令，避免 CPU 进入 busy loop 的悲惨境地）

4) 可以在中断上下文执行。由于不睡眠，因此 spin lock 可以在中断上下文中适用。
{% endhighlight %}

##### 场景分析

对于 spin lock，其保护的资源可能来自多个 CPU CORE 上的进程上下文和中断上下文的中的
访问，其中，进程上下文包括：用户进程通过系统调用访问，内核线程直接访问，来自
workqueue 中 work function 的访问（本质上也是内核线程）。中断上下文包括：
HW interrupt context（中断handler）、软中断上下文（soft irq，当然由于各种原因，
该 softirq 被推迟到 softirqd 的内核线程中执行的时候就不属于这个场景了，属于进程上下
文那个分类了）、timer 的 callback 函数（本质上也是 softirq）、tasklet（本质上也是
softirq）。

先看最简单的单 CPU 上的进程上下文的访问。如果一个全局的资源被多个进程上下文访问，这
时候，内核如何交错执行呢？对于那些没有打开 preemptive 选项的内核，所有的系统调用都
是串行化执行的，因此不存在资源争抢的问题。如果内核线程也访问这个全局资源呢？本质上内
核线程也是进程，类似普通进程，只不过普通进程时而在用户态运行、时而通过系统调用陷入内
核执行，而内核线程永远都是在内核态运行，但是，结果是一样的，对于 non-preemptive 的
linux kernel，只要在内核态，就不会发生进程调度，因此，这种场景下，共享数据根本不需
要保护（没有并发，谈何保护呢）。如果时间停留在这里该多么好，单纯而美好，在继续前进之
前，让我们先享受这一刻。当打开 premptive 选项后，事情变得复杂了，我们考虑下面的场景：

{% highlight ruby %}
1) 进程 A 在某个系统调用过程中访问了共享资源 R

2) 进程 B 在某个系统调用过程中也访问了共享资源 R
{% endhighlight %}

会不会造成冲突呢？假设在 A 访问共享资源 R 的过程中发生了中断，中断唤醒了沉睡中的，优
先级更高的 B，在中断返回现场的时候，发生进程切换，B 启动执行，并通过系统调用访问了 R，
如果没有锁保护，则会出现两个 thread 进入临界区，导致程序执行不正确。OK，我们加上
spin lock 看看如何：A 在进入临界区之前获取了 spin lock，同样的，在 A 访问共享资源 R
的过程中发生了中断，中断唤醒了沉睡中的，优先级更高的 B，B 在访问临界区之前仍然会试图获
取 spin lock，这时候由于 A 进程持有 spin lock 而导致 B 进程进入了永久的 spin……
怎么办？linux 的 kernel 很简单，在 A 进程获取 spin lock 的时候，禁止本 CPU 上的抢
占（上面的永久 spin 的场合仅仅在本 CPU 的进程抢占本 CPU 的当前进程这样的场景中发生）。
如果 A 和 B 运行在不同的 CPU 上，那么情况会简单一些：A 进程虽然持有 spin lock 而导致
B 进程进入 spin 状态，不过由于运行在不同的CPU上，A 进程会持续执行并会很快释放
spin lock，解除 B 进程的 spin状态。

多 CPU core 的场景和单核 CPU 打开 preemptive 选项的效果是一样的，这里不再赘述。继续
向前分析，现在要加入中断上下文这个因素。访问共享资源的 thread 包括：

{% highlight ruby %}
1) 运行在 CPU0 上的进程 A 在某个系统调用过程中访问了共享资源 R

2) 运行在 CPU1 上的进程 B 在某个系统调用过程中也访问了共享资源 R

3) 外设 P 的中断 handler 中也会访问共享资源 R
{% endhighlight %}

在这样的场景下，使用 spin lock 可以保护访问共享资源R的临界区吗？ 假设 CPU0 上的进程
A 持有 spin lock 进入临界区，这时候，外设 P 发生了中断事件，并且调度到了 CPU1 上执行，
看起来没有什么问题，执行在 CPU1 上的 handler 会稍微等待一会 CPU0 上的进程 A，等它立
刻临界区就会释放 spin lock 的，但是，如果外设 P 的中断事件被调度到了 CPU0 上执行会
怎么样？ CPU0 上的进程 A 在持有 spin lock 的状态下被中断上下文抢占，而抢占它的 CPU0
上的 handler 在进入临界区之前仍然会试图获取 spin lock，悲剧发生了，CPU0 上的 P 外设
的中断 handler 永远的进入 spin 状态，这时候，CPU1 上的进程 B 也不可避免在试图持有
spin lock 的时候失败而导致进入 spin 状态。为了解决这样的问题，linux kernel 采用了
这样的办法：如果涉及到中断上下文的访问，spin lock 需要和禁止本 CPU 上的中断联合使用。

linux kernel 中提供了丰富的 bottom half 的机制，虽然同属中断上下文，不过还是稍有不
同。可以把上面的场景简单修改一下：外设 P 不是中断 handler 中访问共享资源 R，而是在的
bottom half 中访问。使用 spin lock + 禁止本地中断当然是可以达到保护共享资源的效果，
但是使用牛刀来杀鸡似乎有点小题大做，这时候 disable bottom half 就 OK 了。

最后，讨论一下中断上下文之间的竞争。同一种中断 handler 之间在 uni core 和 multi
core 上都不会并行执行，这是 linux kernel 的特性。如果不同中断 handler 需要使用
spin lock 保护共享资源，对于新的内核（不区分 fast handler 和 slow handler），
所有 handler 都是关闭中断的，因此使用 spin lock 不需要关闭中断的配合。bottom half
又分成 softirq 和 tasklet，同一种 softirq 会在不同的 CPU 上并发执行，因此如果某个
驱动中的 sofirq 的 handler 中会访问某个全局变量，对该全局变量是需要使用 spin lock
保护的，不用配合 disable CPU 中断或者 bottom half。tasklet 更简单，因为同一种
tasklet 不会多个 CPU 上并发。

### <span id="WFI/WFE/SEV 指令">WFI/WFE/SEV 指令</span>

> [本节原理出自蜗窝科技，原文请戳](http://www.wowotech.net/armv8a_arch/wfe_wfi.html)

WFI (Wait for interrupt) 和 WFE(Wait for event) 是两个让 ARM 核进入
low-power standby 模式的指令，由 ARM architecture 定义，由 ARM core 实现。
SEV (Send Event) 就是向所有处理器发事件信号，将那些等待事件的处理器从 low-power 中唤醒。
《ARMv7 architecture reference manual》如下定义三条指令：

![DTS](/assets/PDB/BiscuitOS/boot/BOOT000062.png)

从上图的定义可知，WFE 指令是让处理器进入一个 lower-pwoer 模式，等待事件信号的唤醒。
其中可以通过 SEV 指令在任意处理器上执行来唤醒该处理器。

![DTS](/assets/PDB/BiscuitOS/boot/BOOT000063.png)

从上图的定义可知，WFI 指令是让处理器进入一个 lower-power 模式，等带同步时间发生。

![DTS](/assets/PDB/BiscuitOS/boot/BOOT000064.png)

从上面的定义可知，SEV 指令会触发一个事件信号，然后通知所有的处理器。

###### WFI 和 WFE 共同点

WFI 和 WFE 的功能非常类似，主要是将 ARMv PE(Processing Element, 处理单元)设置为
"low-power standby state"。需要说明的是，ARM architecture 并没有规定
"low-power standby state" 的具体形式，因而可以由 ARM core 自行发挥，根据 ARM
的建议，一般可以实现为 standby（关闭 clock、保持供电）、dormant、shutdown 等等。
但有个原则，不能造成内存一致性的问题。以 Cortex-A57 ARM core 为例，它把 WFI 和 WFE
实现为 "put the core in a low-power state by disabling the clocks in the core
while keeping the core powered up"，即我们通常所说的 standby 模式，保持供电，关闭
clock。

2）不同点

不同点主要体现进入和退出的方式上。对 WFI 来说，执行 WFI 指令后，ARM core 会立即进入
low-power standby state，直到有 WFI Wakeup events 发生。而 WFE 则稍微不同，执行
WFE 指令后，根据 Event Register（一个单 bit 的寄存器，每个 PE 一个）的状态，有两种
情况：如果 Event Register 为 1，该指令会把它清零，然后执行完成(不会standby);
如果 Event Register 为 0，和 WFI 类似，进入 low-power standby state，直到有
WFE Wakeup events发生。WFI wakeup event 和 WFE wakeup event 可以分别让 Core 从
WFI 和 WFE 状态唤醒，这两类 Event 大部分相同，如任何的 IRQ 中断、FIQ 中断等等，
一些细微的差别。而最大的不同是，WFE 可以被任何 PE 上执行的 SEV 指令唤醒。所谓的 SEV
指令，就是一个用来改变 Event Register 的指令，有两个：SEV 会修改所有 PE 上的寄存器;
SEVL，只修改本地 PE 的寄存器值。下面是 WFE 这种特殊设计的使用场景。

###### WFI

WFI 一般用于 cpuidle。

###### WFE

WFE 的一个典型使用场景，是用在 spinlock 中（可参考arch_spin_lock）。spinlock 的
功能，是在不同 CPU core 之间，保护共享资源。使用 WFE 的流程是：

{% highlight ruby %}
a）资源空闲

b）Core1 访问资源，acquire lock，获得资源

c）Core2 访问资源，此时资源不空闲，执行 WFE 指令，让 core 进入 low-power state

d）Core1 释放资源，release lock，释放资源，同时执行 SEV 指令，唤醒 Core2

e）Core2 获得资源
{% endhighlight %}

以往的 spinlock，在获得不到资源时，让 Core 进入 busy loop，而通过插入 WFE 指令，
可以节省功耗，也算是因祸（损失了性能）得福（降低了功耗）吧。

### <span id="STREX/LDREX 指令">STREX/LDREX 指令</span>

为了实现线程间同步，一般都要在执行关键代码段之前加互斥（Mutex）锁，且在执行完关键
代码段之后解锁。为了实现所谓的互斥锁的概念，一般都需要所在平台提供支持。ARM 平台上
特有的独占访问指令 LDREX 和 STREX。而它们也是 ARM 平台上，实现互斥锁等线程同步工
具的基础。先来看看 LDREX 和 STREX 两条指令的语义。其实 LDREX 和 STREX 指令，是将
单纯的更新内存的原子操作分成了两个独立的步骤。

##### LDREX

LDREX 用来读取内存的时候，并标记对该段内存的独占访问。

{% highlight c %}
LDREX Rx, [Ry]
{% endhighlight %}

上面的指令意味着，读取寄存器 Ry 指向的 4 字节内存值，将其保存到 Rx 寄存器中，同时
标记对 Ry 指向内存区域的独占访问。如果执行 LDREX 指令的时候发现已经被标记为独占访
问了，并不会对指令的执行产生影响。

如果物理地址有共享 TLB 属性，则 LDREX 会将该物理地址标记为由当前处理器独占访问，
并且会清除该处理器对其他任何物理地址的任何独占访问标记。否则，会标记：执行处理器
已经标记了一个物理地址，但访问尚未完毕。

##### STREX

STREX 在更新内存数值时，会检查该段内存是否已经被标记为独占访问，并以此来决定是否更
新内存中的值。

{% highlight c %}
STREX Rx, Ry, [Rz]
{% endhighlight %}

如果执行这条指令的时候发现已经被标记为独占访问了，则将寄存器 Ry 中的值更新到寄存器
Rz 指向的内存，并将寄存器 Rx 设置成 0。指令执行成功后，会将独占访问标记位清除。而如
果执行这条指令的时候发现没有设置独占标记，则不会更新内存，且将寄存器 Rx 的值设置成 1。
一旦某条 STREX 指令执行成功后，以后再对同一段内存尝试使用 STREX 指令更新的时候，会
发现独占标记已经被清空了，就不能再更新了，从而实现独占访问的机制。大致的流程如上，但
是 ARM 内部为了实现这个功能，还有不少复杂的情况要处理。

如果物理地址没有共享 TLB 属性，且执行处理器有一个已标记但尚未访问完毕的物理地址，那
么将会进行存储，清除该标记，并在 Rd 中返回值 0。如果物理地址没有共享 TLB 属性，且
执行处理器也没有已标记但尚未访问完毕的物理地址，那么将不会进行存储，而会在 Rd 中返
回值 1。如果物理地址有共享 TLB 属性，且已被标记为由执行处理器独占访问，那么将进行
存储，清除该标记，并在 Rd 中返回值 0。如果物理地址有共享 TLB 属性，但没有标记为由执
行处理器独占访问，那么不会进行存储，且会在 Rd 中返回值 1。

##### LDREX 和 STREX 使用例子

{% highlight c %}
#include <linux/kernel.h>
#include <linux/init.h>

static volatile unsigned long R0 = 0;

/* atomic_* */
static __init int atomic_demo_init(void)
{
        unsigned long tmp;
        int result;

        __asm__ volatile ("nop\n\t"
                        "ldrex  %0, [%3]\n\t"
                        "add    %0, %0, #9\n\t"
                        "strex  %1, %0, [%3]"
                        : "=&r" (result), "=&r" (tmp), "+Qo" (R0)
                        : "r" (&R0)
                        : "cc");

        printk("R0: %#lx - result: %d - tmp: %#lx\n", R0, result, tmp);

        return 0;
}
device_initcall(atomic_demo_init);
{% endhighlight %}

在上面的内嵌汇编中，首先使用 ldrex 指令从 R0 对应的内存地址中读取数据到 result 中，
此时，ldrex 会将这段区域设置为独占。接着调用 add 指令将 result 的值增加 9. 接着
调用 strex 指令，如果此时独占标志已经置位，那么将 result 的值更新到 R0 对应的内存
里，最后将 tmp 的值设置为 0；如果调用 strex 指令是，发现独占标志没有置位，也就是
有其他线程在写这块物理内存，内存被锁上了，因此 strex 指令不会将 result 的值写入
R0 对应的内存地址上，并将 tmp 的值设置为 1.

上面的代码运行后的结果如下：

{% highlight c %}
usbcore: registered new interface driver usbhid
usbhid: USB HID core driver
R0: 0x9 - result: 9 - tmp: 0x0
aaci-pl041 10004000.aaci: FIFO 512 entries
oprofile: using arm/armv7-ca9
{% endhighlight %}

从上面的执行结果可以看出，ldrex 指令和 strex 指令执行的时候，都独占了 R0 对应的内存。

##### 独占监视器：Exclusive Monitor

在 ARM 系统中，内存有两种不同且对立的属性，即共享 (Shareable) 和非共享
(Non-shareable)。共享意味着该段内存可以被系统中不同处理器访问到，这些处理器可以是同
构的也可以是异构的。而非共享，则相反，意味着该段内存只能被系统中的一个处理器所访问到，
对别的处理器来说不可见。为了实现独占访问，ARM 系统中还特别提供了所谓独占监视器
(Exclusive Monitor) 的东西，其结构大致如下：

![DTS](/assets/PDB/BiscuitOS/boot/BOOT000060.png)

可以看出来，一共有两种类型的独占监视器。每一个处理器内部都有一个本地监视器
(Local Monitor)，且在整个系统范围内还有一个全局监视器 (Global Monitor)。如果要
对非共享内存区中的值进行独占访问，只需要涉及本处理器内部的本地监视器就可以了；而如
果要对共享内存区中的内存进行独占访问，除了要涉及到本处理器内部的本地监视器外，由于
该内存区域可以被系统中所有处理器访问到，因此还必须要由全局监视器来协调。对于本地监
视器来说，它只标记了本处理器对某段内存的独占访问，在调用 LDREX 指令时设置独占访问
标志，在调用 STREX 指令时清除独占访问标志。而对于全局监视器来说，它可以标记每个处
理器对某段内存的独占访问。也就是说，当一个处理器调用 LDREX 访问某段共享内存时，全局
监视器只会设置针对该处理器的独占访问标记，不会影响到其它的处理器。当在以下两种情况下，
会清除某个处理器的独占访问标记：

> 1) 当该处理器调用 LDREX 指令，申请独占访问另一段内存时
>
> 2）当别的处理器成功更新了该段独占访问内存值时

对于第二种情况，也就是说，当独占内存访问内存的值在任何情况下，被任何一个处理器更改过
之后，所有申请独占该段内存的处理器的独占标记都会被清空。另外，更新内存的操作不一定非
要是 STREX 指令，任何其它存储指令都可以。但如果不是 STREX 的话，则没法保证独占访问
性。现在的处理器基本上都是多核的，一个芯片上集成了多个处理器。而且对于一般的操作系统，
系统内存基本上都被设置上了共享属性，也就是说对系统中所有处理器可见。因此，我们这里主
要分析多核系统中对共享内存的独占访问的情况。为了更加清楚的说明，我们可以举一个例子。
假设系统中有两个处理器内核，而一个程序由三个线程组成，其中两个线程被分配到了第一个
处理器上，另外一个线程被分配到了第二个处理器上。如下图，大致经历的步骤如下：

![DTS](/assets/PDB/BiscuitOS/boot/BOOT000061.png)

{% highlight c %}
1) CPU2 上的线程 3 最早执行 LDREX，锁定某段共享内存区域。它会相应更新本地监视
   器和全局监视器。

2) 然后，CPU1 上的线程 1 执行 LDREX，它也会更新本地监视器和全局监视器。这时在全
   局监视器上，CPU1 和 CPU2 都对该段内存做了独占标记。

3) 接着，CPU1 上的线程 2 执行 LDREX 指令，它会发现本处理器的本地监视器对该段内存
   有了独占标记，同时全局监视器上 CPU1 也对该段内存做了独占标记，但这并不会影响这
   条指令的操作。

4) 再下来，CPU1 上的线程 1 最先执行了 STREX 指令，尝试更新该段内存的值。它会发现
   本地监视器对该段内存是有独占标记的，而全局监视器上 CPU1 也有该段内存的独占标记，
   则更新内存值成功。同时，清除本地监视器对该段内存的独占标记，还有全局监视器所有
   处理器对该段内存的独占标记。

5) 下面，CPU2 上的线程 3 执行 STREX 指令，也想更新该段内存值。它会发现本地监视器拥
   有对该段内存的独占标记，但是在全局监视器上 CPU1 没有了该段内存的独占标记（前面一
   步清空了），则更新不成功。

6）最后，CPU1 上的线程 2 执行 STREX 指令，试着更新该段内存值。它会发现本地监视器已
   经没有了对该段内存的独占标记（第4步清除了），则直接更新失败，不需要再查全局监视器了。
{% endhighlight %}

所以，可以看出来，这套机制的精髓就是，无论有多少个处理器，有多少个地方会申请对同
一个内存段进行操作，保证只有最早的更新可以成功，这之后的更新都会失败。失败了就证
明对该段内存有访问冲突了。实际的使用中，可以重新用 LDREX 读取该段内存中保存的最新
值，再处理一次，再尝试保存，直到成功为止。

还有一点需要说明，LDREX 和 STREX 是对内存中的一个字（Word，32 bit）进行独占访问
的指令。如果想独占访问的内存区域不是一个字，还有其它的指令：

{% highlight c %}
1）LDREXB 和 STREXB：对内存中的一个字节（Byte，8 bit）进行独占访问；

2）LDREXH和STREXH：中的一个半字（Half Word，16 bit）进行独占访问；

3）LDREXD和STREXD：中的一个双字（Double Word，64 bit）进行独占访问。
{% endhighlight %}

它们必须配对使用，不能混用。

-----------------------------------

# <span id="源码分析">源码分析</span>

{% highlight ruby %}

{% endhighlight %}


--------------------------------------------------

# <span id="实践">实践</span>

> - [驱动源码](#驱动源码)
>
> - [驱动安装](#驱动安装)
>
> - [驱动配置](#驱动配置)
>
> - [驱动编译](#驱动编译)
>
> - [驱动运行](#驱动运行)
>
> - [驱动分析](#驱动分析)

#### <span id="驱动源码">驱动源码</span>

{% highlight c %}

{% endhighlight %}

#### <span id="驱动安装">驱动安装</span>

驱动的安装很简单，首先将驱动放到 drivers/BiscuitOS/ 目录下，命名为 spinlock.c，
然后修改 Kconfig 文件，添加内容参考如下：

{% highlight bash %}
diff --git a/drivers/BiscuitOS/Kconfig b/drivers/BiscuitOS/Kconfig
index 4edc5a5..1a9abee 100644
--- a/drivers/BiscuitOS/Kconfig
+++ b/drivers/BiscuitOS/Kconfig
@@ -6,4 +6,14 @@ if BISCUITOS_DRV
config BISCUITOS_MISC
        bool "BiscuitOS misc driver"
+config BISCUITOS_SPINLOCK
+       bool "SPINLOCK"
+
+if BISCUITOS_SPINLOCK
+
+config DEBUG_BISCUITOS_SPINLOCK
+       bool "spinlock"
+
+endif # BISCUITOS_SPINLOCK
+
endif # BISCUITOS_DRV
{% endhighlight %}

接着修改 Makefile，请参考如下修改：

{% highlight bash %}
diff --git a/drivers/BiscuitOS/Makefile b/drivers/BiscuitOS/Makefile
index 82004c9..9909149 100644
--- a/drivers/BiscuitOS/Makefile
+++ b/drivers/BiscuitOS/Makefile
@@ -1 +1,2 @@
obj-$(CONFIG_BISCUITOS_MISC)     += BiscuitOS_drv.o
+obj-$(CONFIG_BISCUITOS_SPINLOCK)  += SPINLOCK.o
--
{% endhighlight %}

#### <span id="驱动配置">驱动配置</span>

驱动配置请参考下面文章中关于驱动配置一节。在配置中，勾选如下选项，如下：

{% highlight bash %}
Device Driver--->
    [*]BiscuitOS Driver--->
        [*]SPINLOCK
            [*]spinlock()
{% endhighlight %}

具体过程请参考：

> [Linux 5.0 开发环境搭建 -- 驱动配置](/blog/Linux-5.0-arm32-Usermanual/#%E9%A9%B1%E5%8A%A8%E9%85%8D%E7%BD%AE)

#### <span id="驱动编译">驱动编译</span>

驱动编译也请参考下面文章关于驱动编译一节：

> [Linux 5.0 开发环境搭建 -- 驱动编译](/blog/Linux-5.0-arm32-Usermanual/#%E7%BC%96%E8%AF%91%E9%A9%B1%E5%8A%A8)

#### <span id="驱动运行">驱动运行</span>

驱动的运行，请参考下面文章中关于驱动运行一节：

> [Linux 5.0 开发环境搭建 -- 驱动运行](/blog/Linux-5.0-arm32-Usermanual/#%E9%A9%B1%E5%8A%A8%E8%BF%90%E8%A1%8C)

启动内核，并打印如下信息：

{% highlight ruby %}

{% endhighlight %}

#### <span id="驱动分析">驱动分析</span>

-----------------------------------------------

# <span id="附录">附录</span>

> [BiscuitOS Home](https://biscuitos.github.io/)
>
> [BiscuitOS Driver](/blog/BiscuitOS_Catalogue/)
>
> [BiscuitOS Kernel Build](/blog/Kernel_Build/)
>
> [Linux Kernel](https://www.kernel.org/)
>
> [Bootlin: Elixir Cross Referencer](https://elixir.bootlin.com/linux/latest/source)
>
> [搭建高效的 Linux 开发环境](/blog/Linux-debug-tools/)

## 赞赏一下吧 🙂

![MMU](/assets/PDB/BiscuitOS/kernel/HAB000036.jpg)
